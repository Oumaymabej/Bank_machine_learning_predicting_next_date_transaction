{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb02e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Full inference pipeline: raw -> features -> cluster -> model -> days -> date\n",
    "# ============================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import timedelta\n",
    "import os\n",
    "from typing import Dict, List\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Config\n",
    "# -----------------------------\n",
    "FOURIER_CONFIG = {\n",
    "    \"weekly\":  {\"period\": 7.0,      \"K\": 3},\n",
    "    \"monthly\": {\"period\": 30.4375,  \"K\": 2},\n",
    "    \"yearly\":  {\"period\": 365.25,   \"K\": 3},\n",
    "}\n",
    "\n",
    "# Must match the notebook's keep_cols (you had this exact list)\n",
    "KEEP_COLS = [\n",
    "    'DAYS_SINCE_LAST_TX', 'AMOUNT_LCY', 'AMOUNT_LCY_LOG',\n",
    "    'EXCHANGE_DIFF', 'AMOUNT_RATIO', 'CLIENT_AMOUNT_LCY_mean', \n",
    "    'CLIENT_BOOKING_DATE_count', 'GROUP_AMOUNT_LCY_mean', \n",
    "    'DAYS_SINCE_ACCT_CREATION',\n",
    "    'DESCRIPTION_OPERATION_ENCODED', 'NOM_GROUPE_ENCODED', \n",
    "    'DAY_OF_WEEK', 'WEEK_OF_YEAR', 'TX_NUM', 'TX_REVERSE_NUM',\n",
    "    'QUARTER', 'CLIENT_DAYS_SINCE_LAST_TX_mean',\n",
    "    'CLIENT_AMOUNT_LCY_std',\n",
    "    # Cat columns follow, kept as category for CatBoost\n",
    "    'SENS', 'CURRENCY', 'NATURE_CLIENT', 'DIRECTION', 'SEGMENTS'\n",
    "]\n",
    "\n",
    "CAT_COLS = [\"SENS\", \"CURRENCY\", \"NATURE_CLIENT\", \"DIRECTION\", \"SEGMENTS\",\n",
    "            \"DESCRIPTION_OPERATION_ENCODED\", \"NOM_GROUPE_ENCODED\"]\n",
    "\n",
    "FRM_SCALER_PATH = \"scaler.pkl\"\n",
    "KMEANS_PATH     = \"kmeans.pkl\"\n",
    "CATBOOST_PATH   = \"CatBoost.pkl\"\n",
    "RIDGE_PATH      = \"Ridge.pkl\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Utils\n",
    "# -----------------------------\n",
    "def _relative_days(dates, ref_date):\n",
    "    s = pd.Series(dates)\n",
    "    if pd.api.types.is_datetime64_any_dtype(s):\n",
    "        delta = s - pd.Timestamp(ref_date)\n",
    "    elif pd.api.types.is_timedelta64_dtype(s):\n",
    "        delta = s\n",
    "    else:\n",
    "        s = pd.to_datetime(s, errors=\"coerce\")\n",
    "        delta = s - pd.Timestamp(ref_date)\n",
    "    return (delta / pd.Timedelta(days=1)).to_numpy(dtype=float)\n",
    "\n",
    "def make_fourier_df(dates, config, ref_date):\n",
    "    t = _relative_days(dates, ref_date)\n",
    "    feats = {}\n",
    "    for name, cfg in config.items():\n",
    "        P, K = cfg[\"period\"], cfg[\"K\"]\n",
    "        for k in range(1, K + 1):\n",
    "            feats[f\"{name}_sin_{k}\"] = np.sin(2.0 * np.pi * k * t / P)\n",
    "            feats[f\"{name}_cos_{k}\"] = np.cos(2.0 * np.pi * k * t / P)\n",
    "    return pd.DataFrame(feats, index=pd.RangeIndex(len(t)))\n",
    "\n",
    "def encode_categorical_on_full(df: pd.DataFrame):\n",
    "    # Low-card cols + two label-encoded texts (as in your notebook)\n",
    "    cat_cols = ['SENS', 'CURRENCY', 'NATURE_CLIENT', 'DIRECTION', 'SEGMENTS']\n",
    "    for col in ['DESCRIPTION_OPERATION', 'NOM_GROUPE']:\n",
    "        le = LabelEncoder()\n",
    "        df[col + '_ENCODED'] = le.fit_transform(df[col].astype(str))\n",
    "        cat_cols.append(col + '_ENCODED')\n",
    "    # cast to category dtypes for CatBoost\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df: pd.DataFrame):\n",
    "    # Date handling (expects BOOKING_DATE, AJOUTE_LE already datetime)\n",
    "    df = df.sort_values(['CLIENT_ID', 'BOOKING_DATE']).copy()\n",
    "    df['YEAR'] = df['BOOKING_DATE'].dt.year\n",
    "    df['MONTH'] = df['BOOKING_DATE'].dt.month\n",
    "    df['DAY'] = df['BOOKING_DATE'].dt.day\n",
    "    df['DAY_OF_WEEK'] = df['BOOKING_DATE'].dt.dayofweek\n",
    "    df['WEEK_OF_YEAR'] = df['BOOKING_DATE'].dt.isocalendar().week.astype('int32')\n",
    "    df['IS_MONTH_START'] = df['BOOKING_DATE'].dt.is_month_start.astype(int)\n",
    "    df['IS_MONTH_END'] = df['BOOKING_DATE'].dt.is_month_end.astype(int)\n",
    "    df['QUARTER'] = df['BOOKING_DATE'].dt.quarter\n",
    "\n",
    "    # Time-based\n",
    "    df['DAYS_SINCE_ACCT_CREATION'] = (df['BOOKING_DATE'] - df['AJOUTE_LE']).dt.days\n",
    "    df['DAYS_SINCE_LAST_TX'] = df.groupby('CLIENT_ID')['BOOKING_DATE'].diff().dt.days\n",
    "\n",
    "    # Amount features (same as notebook)\n",
    "    df['AMOUNT_RATIO'] = df['AMOUNT_FCY'] / df['AMOUNT_LCY']\n",
    "    df['EXCHANGE_DIFF'] = df['EXCHANGE_RATE'] - df['COURS_MARCHE']\n",
    "    df['AMOUNT_SIGN'] = np.where(df['SENS'] == 'A', 1, -1)\n",
    "    df['AMOUNT_SIZE'] = np.log1p(np.abs(df['AMOUNT_LCY']))\n",
    "    df['AMOUNT_LCY_LOG'] = np.log1p(df['AMOUNT_LCY'])\n",
    "\n",
    "    # Client agg\n",
    "    client_agg = df.groupby('CLIENT_ID').agg({\n",
    "        'AMOUNT_LCY': ['mean', 'std', 'sum', 'min', 'max', 'count'],\n",
    "        'DAYS_SINCE_LAST_TX': ['mean', 'std', 'median'],\n",
    "        'BOOKING_DATE': ['min', 'max', 'count'],\n",
    "    }).reset_index()\n",
    "    client_agg.columns = ['CLIENT_' + '_'.join(col).strip('_') for col in client_agg.columns.values]\n",
    "    client_agg = client_agg.rename(columns={'CLIENT_CLIENT_ID': 'CLIENT_ID'})\n",
    "    df = pd.merge(df, client_agg, on='CLIENT_ID', how='left')\n",
    "\n",
    "    # Group agg\n",
    "    group_agg = df.groupby('NOM_GROUPE').agg({\n",
    "        'AMOUNT_LCY': ['mean', 'median', 'std'],\n",
    "        'BOOKING_DATE': ['count'],\n",
    "    }).reset_index()\n",
    "    group_agg.columns = ['GROUP_' + '_'.join(col).strip('_') for col in group_agg.columns.values]\n",
    "    group_agg = group_agg.rename(columns={'GROUP_NOM_GROUPE': 'NOM_GROUPE'})\n",
    "    df = pd.merge(df, group_agg, on='NOM_GROUPE', how='left')\n",
    "\n",
    "    # Segment agg (count-only to avoid target leakage)\n",
    "    segment_agg = df.groupby('SEGMENTS').agg({'BOOKING_DATE': ['count']}).reset_index()\n",
    "    segment_agg.columns = ['SEGMENT_' + '_'.join(col).strip('_') for col in segment_agg.columns.values]\n",
    "    segment_agg = segment_agg.rename(columns={'SEGMENT_SEGMENTS': 'SEGMENTS'})\n",
    "    df = pd.merge(df, segment_agg, on='SEGMENTS', how='left')\n",
    "\n",
    "    # Sequencing\n",
    "    df['TX_NUM'] = df.groupby('CLIENT_ID').cumcount() + 1\n",
    "    df['TX_REVERSE_NUM'] = df.groupby('CLIENT_ID')['BOOKING_DATE'].rank(ascending=False, method='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_missing_numeric(df: pd.DataFrame):\n",
    "    # DAYS_SINCE_LAST_TX: fill per-client median (or 0 for first tx)\n",
    "    first_tx_mask = df.groupby('CLIENT_ID')['BOOKING_DATE'].rank(method='first') == 1\n",
    "    df['DAYS_SINCE_LAST_TX'] = (\n",
    "        df.groupby('CLIENT_ID')['DAYS_SINCE_LAST_TX']\n",
    "          .transform(lambda s: s.fillna(s.median() if not pd.isna(s.median()) else 0))\n",
    "    )\n",
    "    df.loc[first_tx_mask, 'DAYS_SINCE_LAST_TX'] = 0\n",
    "\n",
    "    # general numeric fills\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype != 'category' and pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if df[col].isnull().any():\n",
    "                if col.startswith(('CLIENT_', 'GROUP_', 'SEGMENT_')):\n",
    "                    df[col] = df[col].fillna(df[col].median())\n",
    "                else:\n",
    "                    df[col] = df[col].fillna(0)\n",
    "    return df\n",
    "\n",
    "def build_frm_clusters(train_df: pd.DataFrame):\n",
    "    # FRM on train only\n",
    "    frm = train_df.groupby('CLIENT_ID').agg({\n",
    "        'BOOKING_DATE': 'count',            # Frequency\n",
    "        'AMOUNT_LCY': 'mean',               # Monetary\n",
    "        'DAYS_SINCE_LAST_TX': 'mean',       # Recency proxy\n",
    "    }).reset_index()\n",
    "    frm.columns = ['CLIENT_ID', 'TX_FREQUENCY', 'TX_AMOUNT_MEAN', 'TX_RECENCY']\n",
    "    frm['TX_AMOUNT_MEAN_LOG'] = np.log1p(np.abs(frm['TX_AMOUNT_MEAN']))\n",
    "\n",
    "    # load if available; else fit & save\n",
    "    if os.path.exists(FRM_SCALER_PATH):\n",
    "        scaler = joblib.load(FRM_SCALER_PATH)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        joblib.dump(scaler, FRM_SCALER_PATH)\n",
    "    scaled = scaler.fit_transform(frm[['TX_FREQUENCY', 'TX_AMOUNT_MEAN_LOG', 'TX_RECENCY']])\n",
    "\n",
    "    if os.path.exists(KMEANS_PATH):\n",
    "        kmeans = joblib.load(KMEANS_PATH)\n",
    "    else:\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "        joblib.dump(kmeans, KMEANS_PATH)\n",
    "\n",
    "    frm['CLUSTER'] = kmeans.fit_predict(scaled)\n",
    "    return frm[['CLIENT_ID', 'CLUSTER']], scaler, kmeans\n",
    "\n",
    "def prepare_ridge_inputs(X_train: pd.DataFrame, X_test: pd.DataFrame):\n",
    "    \"\"\"Replicate Ridge training preprocessing: encode categoricals, scale numerics.\"\"\"\n",
    "    X_tr = X_train.copy()\n",
    "    X_te = X_test.copy()\n",
    "    # Fit per-column LabelEncoder on combined to avoid unknowns at inference\n",
    "    for col in X_tr.columns:\n",
    "        if X_tr[col].dtype.name in (\"category\", \"object\") or isinstance(X_tr[col].dtype, pd.CategoricalDtype):\n",
    "            le = LabelEncoder()\n",
    "            vals = pd.concat([X_tr[col].astype(str), X_te[col].astype(str)], axis=0)\n",
    "            le.fit(vals)\n",
    "            X_tr[col] = le.transform(X_tr[col].astype(str))\n",
    "            X_te[col] = le.transform(X_te[col].astype(str))\n",
    "    scaler = StandardScaler()\n",
    "    X_tr_sc = scaler.fit_transform(X_tr)\n",
    "    X_te_sc = scaler.transform(X_te)\n",
    "    return X_tr_sc, X_te_sc, scaler\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Main entry\n",
    "# -----------------------------\n",
    "def predict_next_dates(train_raw: pd.DataFrame, test_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Parse datetimes\n",
    "    for df in [train_raw, test_raw]:\n",
    "        df['BOOKING_DATE'] = pd.to_datetime(df['BOOKING_DATE'])\n",
    "        df['AJOUTE_LE']     = pd.to_datetime(df['AJOUTE_LE'])\n",
    "\n",
    "    # Combine for FE (like your notebook), then re-split\n",
    "    full = pd.concat([train_raw.copy(), test_raw.copy()], ignore_index=True)\n",
    "    full = full.sort_values(['CLIENT_ID', 'BOOKING_DATE'])\n",
    "\n",
    "    # Build NEXT date gap to compute target for train rows (if present)\n",
    "    full['NEXT_BOOKING_DATE'] = full.groupby('CLIENT_ID')['BOOKING_DATE'].shift(-1)\n",
    "    full['DAYS_TO_NEXT'] = (full['NEXT_BOOKING_DATE'] - full['BOOKING_DATE']).dt.days\n",
    "\n",
    "    # Feature eng + categorical encoding\n",
    "    full = feature_engineering(full)\n",
    "    full = encode_categorical_on_full(full)\n",
    "\n",
    "    # Split back to train/test by presence of target\n",
    "    train_fe = full[full['DAYS_TO_NEXT'].notnull()].copy()\n",
    "    test_fe  = full[full['DAYS_TO_NEXT'].isnull()].copy()\n",
    "\n",
    "    # Fill NaNs in numerics\n",
    "    train_fe = fill_missing_numeric(train_fe)\n",
    "    test_fe  = fill_missing_numeric(test_fe)\n",
    "\n",
    "    # FRM clustering using train only\n",
    "    client_clusters, scaler, kmeans = build_frm_clusters(train_fe)\n",
    "\n",
    "    # Attach cluster to both\n",
    "    train_fe = train_fe.merge(client_clusters, on='CLIENT_ID', how='left')\n",
    "    test_fe  = test_fe.merge(client_clusters,  on='CLIENT_ID', how='left')\n",
    "\n",
    "    # Unseen clients in test → most frequent cluster\n",
    "    if test_fe['CLUSTER'].isnull().any():\n",
    "        most_freq_cluster = train_fe['CLUSTER'].mode()[0]\n",
    "        test_fe['CLUSTER'] = test_fe['CLUSTER'].fillna(most_freq_cluster)\n",
    "\n",
    "    # Targets (log) for consistency if we ever need train stats\n",
    "    train_fe['DAYS_TO_NEXT_LOG'] = np.log1p(train_fe['DAYS_TO_NEXT'])\n",
    "\n",
    "    # ----------------- Cluster 0 → CatBoost (+Fourier) -----------------\n",
    "    cb_model = joblib.load(CATBOOST_PATH)\n",
    "\n",
    "    test_c0 = test_fe[test_fe['CLUSTER'] == 0].copy()\n",
    "    train_c0 = train_fe[train_fe['CLUSTER'] == 0].copy()\n",
    "\n",
    "    # Build design matrix in the SAME ORDER used in training: keep_cols then Fourier\n",
    "    X0_train_base = train_c0[KEEP_COLS].copy()\n",
    "    X0_test_base  = test_c0[KEEP_COLS].copy()\n",
    "\n",
    "    # ensure category dtypes are intact for CatBoost\n",
    "    for c in CAT_COLS:\n",
    "        if c in X0_train_base.columns:\n",
    "            X0_train_base[c] = X0_train_base[c].astype('category')\n",
    "        if c in X0_test_base.columns:\n",
    "            X0_test_base[c] = X0_test_base[c].astype('category')\n",
    "\n",
    "    # Reference date = min training date for seasonality phase (same logic as notebook)\n",
    "    ref_date_c0 = pd.to_datetime(train_c0['BOOKING_DATE']).min()\n",
    "    fourier_train = make_fourier_df(train_c0['BOOKING_DATE'], FOURIER_CONFIG, ref_date_c0)\n",
    "    fourier_test  = make_fourier_df(test_c0['BOOKING_DATE'], FOURIER_CONFIG, ref_date_c0)\n",
    "\n",
    "    X0_train_aug = pd.concat([X0_train_base.reset_index(drop=True), fourier_train.reset_index(drop=True)], axis=1)\n",
    "    X0_test_aug  = pd.concat([X0_test_base.reset_index(drop=True),  fourier_test.reset_index(drop=True)],  axis=1)\n",
    "\n",
    "    # Align cat categories to training baseline (avoid unseen-category blowups)\n",
    "    cat_cols_present = [c for c in CAT_COLS if c in X0_train_aug.columns]\n",
    "    base_cats = {c: X0_train_aug[c].cat.categories for c in cat_cols_present}\n",
    "    for c in cat_cols_present:\n",
    "        X0_test_aug[c] = X0_test_aug[c].astype('category').cat.set_categories(base_cats[c])\n",
    "\n",
    "    # Predict log-days -> real days\n",
    "    pred_c0_log = cb_model.predict(X0_test_aug)\n",
    "    pred_c0 = np.expm1(pred_c0_log).astype(float)\n",
    "    pred_c0 = np.clip(pred_c0, 0.0, None)\n",
    "    pred_c0_days_int = np.ceil(pred_c0).astype(int)\n",
    "    test_c0['pred_days'] = pred_c0\n",
    "    test_c0['pred_days_int'] = pred_c0_days_int\n",
    "    test_c0['predicted_next_date'] = pd.to_datetime(test_c0['BOOKING_DATE']) + pd.to_timedelta(test_c0['pred_days_int'], unit='D')\n",
    "\n",
    "    # ----------------- Cluster 1 → Ridge -----------------\n",
    "    ridge_model = joblib.load(RIDGE_PATH)\n",
    "\n",
    "    test_c1 = test_fe[test_fe['CLUSTER'] == 1].copy()\n",
    "    train_c1 = train_fe[train_fe['CLUSTER'] == 1].copy()\n",
    "\n",
    "    X1_train = train_c1[KEEP_COLS].copy()\n",
    "    X1_test  = test_c1[KEEP_COLS].copy()\n",
    "\n",
    "    # Ridge training in your notebook:\n",
    "    # - LabelEncode every categorical/object col\n",
    "    # - StandardScale all features\n",
    "    X1_train_sc, X1_test_sc, scaler_ridge = prepare_ridge_inputs(X1_train, X1_test)\n",
    "\n",
    "    pred_c1_log = ridge_model.predict(X1_test_sc)\n",
    "    pred_c1 = np.expm1(pred_c1_log).astype(float)\n",
    "    pred_c1 = np.clip(pred_c1, 0.0, None)\n",
    "    pred_c1_days_int = np.ceil(pred_c1).astype(int)\n",
    "    test_c1['pred_days'] = pred_c1\n",
    "    test_c1['pred_days_int'] = pred_c1_days_int\n",
    "    test_c1['predicted_next_date'] = pd.to_datetime(test_c1['BOOKING_DATE']) + pd.to_timedelta(test_c1['pred_days_int'], unit='D')\n",
    "\n",
    "    # ----------------- Combine & return -----------------\n",
    "    out = pd.concat([test_c0, test_c1], axis=0).sort_index()\n",
    "    # nice minimal output; add more columns if you want\n",
    "    cols = [\n",
    "        'REFERENCE_OPERATION_T24', 'CLIENT_ID', 'BOOKING_DATE',\n",
    "        'CLUSTER', 'pred_days', 'pred_days_int', 'predicted_next_date'\n",
    "    ]\n",
    "    cols = [c for c in cols if c in out.columns]\n",
    "    return out[cols].reset_index(drop=True)\n",
    "\n",
    "# ============================\n",
    "# Example usage\n",
    "# ============================\n",
    "# train = ...  # your raw train df (columns as you listed)\n",
    "# test  = ...  # your raw test df\n",
    "# preds = predict_next_dates(train, test)\n",
    "# display(preds.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
